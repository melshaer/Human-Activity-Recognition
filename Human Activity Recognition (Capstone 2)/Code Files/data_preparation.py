# -*- coding: utf-8 -*-
"""data_preparation.py

Automatically generated by Colaboratory.
Edited by Menna El-Shaer for local development (8/7/20)

Original file is located at
    https://colab.research.google.com/drive/1OGMQj1VYCutNLQhA8FBIqLv9cnorzTjK
"""

from pathlib import Path
from scipy.io import loadmat
import numpy as np
import json
from json import JSONEncoder
from PIL import Image


# function to check for missing annotation data entries
def is_empty(list, name):
    try:
        list[name]
    except ValueError:
        return True

    if len(list[name]) > 0:
        return False
    else:
        return True


# Adapted from https://github.com/mks0601/TF-SimpleHumanPose/blob/master/tool/mpii2coco.py

# images_dir_path_str = 'gdrive/My Drive/images/'
# annot_file_path_str = 'gdrive/My Drive/mpii_human_pose_v1_u12_1.mat'

images_dir_path = Path('images/')
annot_file_path_str = 'mpii_human_pose_v1_u12_1.mat'
data_type = 'train'
save_path = data_type + '_subset.json'

annot_data = loadmat(annot_file_path_str)['RELEASE']

num_joints = 16
num_images = len(annot_data['annolist'][0][0][0])

dataset = {'images': [], 'activities': [], 'annotations': []}

for imgidx in range(num_images):
    print(imgidx)
    if ((data_type == 'train' and annot_data['img_train'][0][0][0][imgidx] == 1) or (
            data_type == 'test' and annot_data['img_train'][0][0][0][imgidx] == 0)) and is_empty(
        annot_data['annolist'][0][0][0][imgidx], 'annorect') == False:  # any person is annotated
        # open corresponding image
        img_filename = str(images_dir_path) + '/' + str(
            annot_data['annolist'][0][0][0][imgidx]['image'][0][0][0][0])  # image filename
        try:
            image = Image.open(img_filename)
            w, h = image.size

            image_dict = {'id': imgidx, 'file_name': img_filename, 'width': w, 'height': h}

            # add image to dataset
            dataset['images'].append(image_dict)

            # don't add annotations or activities to test dataset (will be used as ground truth later)
            if data_type == 'test':
                continue

            num_persons = len(annot_data['annolist'][0][0][0][imgidx]['annorect'][0])  # number of people in image
            joint_annotated = np.zeros((num_persons, num_joints))

            for ridx in range(num_persons):
                if is_empty(annot_data['annolist'][0][0][0][imgidx]['annorect'][0][ridx],
                            'annopoints') == False:  # is annotated
                    bbox = np.zeros((4))  # xmin, ymin, width, height
                    keypts = np.zeros((num_joints, 3))  # x, y, is_visible

                    # extract annotated keypoints
                    num_annot_joints = len(
                        annot_data['annolist'][0][0][0][imgidx]['annorect'][0][ridx]['annopoints']['point'][0][0][0])
                    for jidx in range(num_annot_joints):
                        annot_jid = \
                            annot_data['annolist'][0][0][0][imgidx]['annorect'][0][ridx]['annopoints']['point'][0][0][
                                0][
                                jidx]['id'][0][0]
                        keypts[annot_jid][0] = \
                            annot_data['annolist'][0][0][0][imgidx]['annorect'][0][ridx]['annopoints']['point'][0][0][
                                0][
                                jidx]['x'][0][0]  # x
                        keypts[annot_jid][1] = \
                            annot_data['annolist'][0][0][0][imgidx]['annorect'][0][ridx]['annopoints']['point'][0][0][
                                0][
                                jidx]['y'][0][0]  # y
                        keypts[annot_jid][2] = 1  # is_visible

                        # extract bbox from visible annotated keypoints
                        annot_keypts = keypts[keypts[:, 2] == 1, :].reshape(-1, 3)

                        xmin = np.min(annot_keypts[:, 0])
                        ymin = np.min(annot_keypts[:, 1])
                        xmax = np.max(annot_keypts[:, 0])
                        ymax = np.max(annot_keypts[:, 1])

                        width = xmax - xmin - 1
                        height = ymax - ymin - 1

                        # ignore corrupted bbox
                        if width <= 0 or height <= 0:
                            continue
                        else:
                            # 20% extend bbox dimensions
                            bbox[0] = (xmin + xmax) / 2. - width / 2 * 1.2
                            bbox[1] = (ymin + ymax) / 2. - height / 2 * 1.2
                            bbox[2] = width * 1.2
                            bbox[3] = height * 1.2

                person_dict = {'person_id': ridx, 'image_id': imgidx, 'area': bbox[2] * bbox[3], 'bbox': bbox.tolist(),
                               'keypoints_x': keypts[:, 0].tolist(),
                               'keypoints_y': keypts[:, 1].tolist(), 'keypoints_vis': keypts[:, 2].tolist(),
                               'num_keypoints': int(np.sum(keypts[:, 2] == 1))}

                # add annotations to dataset
                dataset['annotations'].append(person_dict)

            print(imgidx)
            if is_empty(annot_data['act'][0][0][imgidx], 'act_name') == False:
                activity = annot_data['act'][0][0][imgidx]

                activity_dict = {'image_id': imgidx, 'activity_name': activity['act_name'].tolist(),
                                 'category_name': activity['cat_name'].tolist(),
                                 'activity_id': activity['act_id'].tolist()}

                # add activity to dataset
                dataset['activities'].append(activity_dict)
        except:
            pass


class NumpyArrayEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return JSONEncoder.default(self, obj)


with open(save_path, 'w') as f:
    json.dump(dataset, f, cls=NumpyArrayEncoder)
